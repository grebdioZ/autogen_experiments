{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "842d4017",
   "metadata": {},
   "source": [
    "# Basic Multi-Agent Example with AutoGen\n",
    "\n",
    "This notebook implements a simple multi-agent system using AutoGen. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d8054ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "import asyncio\n",
    "import timeit\n",
    "from autogen_agentchat.agents import AssistantAgent\n",
    "from autogen_agentchat.teams import RoundRobinGroupChat\n",
    "from autogen_agentchat.conditions import MaxMessageTermination\n",
    "import sys\n",
    "\n",
    "sys.path.append(os.path.abspath(\"..\"))\n",
    "from utilities import (\n",
    "    create_chat_completion_client,    \n",
    "    adjust_properties_for_fixed_response,    \n",
    "    load_model_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38c88307",
   "metadata": {},
   "source": [
    "#### Note: You need a tavily API key to use the web search tool!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b6c1917",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using model: qwen3 type: ollama\n"
     ]
    }
   ],
   "source": [
    "# model = load_model_config()\n",
    "model = load_model_config(model_name=\"gpt-oss:20b\")\n",
    "# model = load_model_config(model_name=\"qwen3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5375bd1f",
   "metadata": {},
   "source": [
    "Set the joke's topic\n",
    "----------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1be891ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic = \"Animals\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d2fdaa3",
   "metadata": {},
   "source": [
    "Select Model and Agent Configuration\n",
    "-----------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cce96c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "agentConfig = {\n",
    "    \"Author\": dict(\n",
    "        system_message=\"Write a SHORT joke above the provided topic.\"\n",
    "    ),\n",
    "    \"SmartassEditor\": dict(\n",
    "        system_message=\"Analyze the joke, explain why it is funny or not funny, and make it funnier without making it longer.\",\n",
    "    ),\n",
    "}\n",
    "\n",
    "# For testing, we can fix an agent's response to a known answer\n",
    "#adjust_properties_for_fixed_response(agentConfig[\"Author\"], \"What do you call a magic dog? A Labracadabrador.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b1e278d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_config {'name': 'qwen3', 'type': 'ollama', 'host': 'https://ollama.cloud.intern.mevis.fraunhofer.de'}\n"
     ]
    }
   ],
   "source": [
    "# Now create model client\n",
    "model_client = create_chat_completion_client(model)\n",
    "\n",
    "agents = []  # start with a user proxy for the researcher\n",
    "\n",
    "for agent_type, props in agentConfig.items():\n",
    "    agents.append(\n",
    "        AssistantAgent(\n",
    "            name=agent_type,\n",
    "            **props,  # type: ignore\n",
    "            model_client=model_client,\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bcf867c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up termination conditions\n",
    "termination = MaxMessageTermination(max_messages=10) #len(agents))\n",
    "\n",
    "# Create the team\n",
    "team = RoundRobinGroupChat(\n",
    "    participants=agents,\n",
    "    termination_condition=termination,\n",
    "    max_turns=len(agents) # we don't really want it to go round\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92697f14",
   "metadata": {},
   "source": [
    "Start the System\n",
    "==================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6085732d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error processing publish message for Author_5a59898f-9289-4e29-bcbf-729095be1acc/5a59898f-9289-4e29-bcbf-729095be1acc\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\dev\\git\\autogen_experiments\\.venv\\Lib\\site-packages\\autogen_core\\_single_threaded_agent_runtime.py\", line 606, in _on_message\n",
      "    return await agent.on_message(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    ...<2 lines>...\n",
      "    )\n",
      "    ^\n",
      "  File \"c:\\dev\\git\\autogen_experiments\\.venv\\Lib\\site-packages\\autogen_core\\_base_agent.py\", line 119, in on_message\n",
      "    return await self.on_message_impl(message, ctx)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\dev\\git\\autogen_experiments\\.venv\\Lib\\site-packages\\autogen_agentchat\\teams\\_group_chat\\_sequential_routed_agent.py\", line 67, in on_message_impl\n",
      "    return await super().on_message_impl(message, ctx)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\dev\\git\\autogen_experiments\\.venv\\Lib\\site-packages\\autogen_core\\_routed_agent.py\", line 485, in on_message_impl\n",
      "    return await h(self, message, ctx)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\dev\\git\\autogen_experiments\\.venv\\Lib\\site-packages\\autogen_core\\_routed_agent.py\", line 268, in wrapper\n",
      "    return_value = await func(self, message, ctx)  # type: ignore\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\dev\\git\\autogen_experiments\\.venv\\Lib\\site-packages\\autogen_agentchat\\teams\\_group_chat\\_chat_agent_container.py\", line 133, in handle_request\n",
      "    async for msg in self._agent.on_messages_stream(self._message_buffer, ctx.cancellation_token):\n",
      "    ...<4 lines>...\n",
      "            await self._log_message(msg)\n",
      "  File \"c:\\dev\\git\\autogen_experiments\\.venv\\Lib\\site-packages\\autogen_agentchat\\agents\\_assistant_agent.py\", line 953, in on_messages_stream\n",
      "    async for inference_output in self._call_llm(\n",
      "    ...<15 lines>...\n",
      "            yield inference_output\n",
      "  File \"c:\\dev\\git\\autogen_experiments\\.venv\\Lib\\site-packages\\autogen_agentchat\\agents\\_assistant_agent.py\", line 1107, in _call_llm\n",
      "    model_result = await model_client.create(\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    ...<4 lines>...\n",
      "    )\n",
      "    ^\n",
      "  File \"c:\\dev\\git\\autogen_experiments\\.venv\\Lib\\site-packages\\autogen_ext\\models\\ollama\\_ollama_client.py\", line 646, in create\n",
      "    result: ChatResponse = await future\n",
      "                           ^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\jmkuhnigk\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\asyncio\\futures.py\", line 286, in __await__\n",
      "    yield self  # This tells Task to wait for completion.\n",
      "    ^^^^^^^^^^\n",
      "  File \"C:\\Users\\jmkuhnigk\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\asyncio\\tasks.py\", line 375, in __wakeup\n",
      "    future.result()\n",
      "    ~~~~~~~~~~~~~^^\n",
      "  File \"C:\\Users\\jmkuhnigk\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\asyncio\\futures.py\", line 199, in result\n",
      "    raise self._exception.with_traceback(self._exception_tb)\n",
      "  File \"C:\\Users\\jmkuhnigk\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\asyncio\\tasks.py\", line 304, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "  File \"c:\\dev\\git\\autogen_experiments\\.venv\\Lib\\site-packages\\ollama\\_client.py\", line 854, in chat\n",
      "    return await self._request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "    ...<14 lines>...\n",
      "    )\n",
      "    ^\n",
      "  File \"c:\\dev\\git\\autogen_experiments\\.venv\\Lib\\site-packages\\ollama\\_client.py\", line 692, in _request\n",
      "    return cls(**(await self._request_raw(*args, **kwargs)).json())\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\dev\\git\\autogen_experiments\\.venv\\Lib\\site-packages\\ollama\\_client.py\", line 636, in _request_raw\n",
      "    raise ResponseError(e.response.text, e.response.status_code) from None\n",
      "ollama._types.ResponseError: model 'qwen3' not found (status code: 404)\n",
      "Error processing publish message for SmartassEditor_5a59898f-9289-4e29-bcbf-729095be1acc/5a59898f-9289-4e29-bcbf-729095be1acc\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\dev\\git\\autogen_experiments\\.venv\\Lib\\site-packages\\autogen_core\\_single_threaded_agent_runtime.py\", line 606, in _on_message\n",
      "    return await agent.on_message(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    ...<2 lines>...\n",
      "    )\n",
      "    ^\n",
      "  File \"c:\\dev\\git\\autogen_experiments\\.venv\\Lib\\site-packages\\autogen_core\\_base_agent.py\", line 119, in on_message\n",
      "    return await self.on_message_impl(message, ctx)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\dev\\git\\autogen_experiments\\.venv\\Lib\\site-packages\\autogen_agentchat\\teams\\_group_chat\\_sequential_routed_agent.py\", line 72, in on_message_impl\n",
      "    return await super().on_message_impl(message, ctx)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\dev\\git\\autogen_experiments\\.venv\\Lib\\site-packages\\autogen_core\\_routed_agent.py\", line 486, in on_message_impl\n",
      "    return await self.on_unhandled_message(message, ctx)  # type: ignore\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\dev\\git\\autogen_experiments\\.venv\\Lib\\site-packages\\autogen_agentchat\\teams\\_group_chat\\_chat_agent_container.py\", line 195, in on_unhandled_message\n",
      "    raise ValueError(f\"Unhandled message in agent container: {type(message)}\")\n",
      "ValueError: Unhandled message in agent container: <class 'autogen_agentchat.teams._group_chat._events.GroupChatError'>\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "ResponseError: model 'qwen3' not found (status code: 404)\nTraceback:\nTraceback (most recent call last):\n\n  File \"c:\\dev\\git\\autogen_experiments\\.venv\\Lib\\site-packages\\autogen_agentchat\\teams\\_group_chat\\_chat_agent_container.py\", line 133, in handle_request\n    async for msg in self._agent.on_messages_stream(self._message_buffer, ctx.cancellation_token):\n    ...<4 lines>...\n            await self._log_message(msg)\n\n  File \"c:\\dev\\git\\autogen_experiments\\.venv\\Lib\\site-packages\\autogen_agentchat\\agents\\_assistant_agent.py\", line 953, in on_messages_stream\n    async for inference_output in self._call_llm(\n    ...<15 lines>...\n            yield inference_output\n\n  File \"c:\\dev\\git\\autogen_experiments\\.venv\\Lib\\site-packages\\autogen_agentchat\\agents\\_assistant_agent.py\", line 1107, in _call_llm\n    model_result = await model_client.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^\n    ...<4 lines>...\n    )\n    ^\n\n  File \"c:\\dev\\git\\autogen_experiments\\.venv\\Lib\\site-packages\\autogen_ext\\models\\ollama\\_ollama_client.py\", line 646, in create\n    result: ChatResponse = await future\n                           ^^^^^^^^^^^^\n\n  File \"C:\\Users\\jmkuhnigk\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\asyncio\\futures.py\", line 286, in __await__\n    yield self  # This tells Task to wait for completion.\n    ^^^^^^^^^^\n\n  File \"C:\\Users\\jmkuhnigk\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\asyncio\\tasks.py\", line 375, in __wakeup\n    future.result()\n    ~~~~~~~~~~~~~^^\n\n  File \"C:\\Users\\jmkuhnigk\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\asyncio\\futures.py\", line 199, in result\n    raise self._exception.with_traceback(self._exception_tb)\n\n  File \"C:\\Users\\jmkuhnigk\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\asyncio\\tasks.py\", line 304, in __step_run_and_handle_result\n    result = coro.send(None)\n\n  File \"c:\\dev\\git\\autogen_experiments\\.venv\\Lib\\site-packages\\ollama\\_client.py\", line 854, in chat\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n    ...<14 lines>...\n    )\n    ^\n\n  File \"c:\\dev\\git\\autogen_experiments\\.venv\\Lib\\site-packages\\ollama\\_client.py\", line 692, in _request\n    return cls(**(await self._request_raw(*args, **kwargs)).json())\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n  File \"c:\\dev\\git\\autogen_experiments\\.venv\\Lib\\site-packages\\ollama\\_client.py\", line 636, in _request_raw\n    raise ResponseError(e.response.text, e.response.status_code) from None\n\nollama._types.ResponseError: model 'qwen3' not found (status code: 404)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      6\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m result  \u001b[38;5;66;03m# Return the result for further exploration\u001b[39;00m\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# Run the deep dive\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m result = \u001b[38;5;28;01mawait\u001b[39;00m invent_joke(topic)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 3\u001b[39m, in \u001b[36minvent_joke\u001b[39m\u001b[34m(topic)\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minvent_joke\u001b[39m(topic: \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m      2\u001b[39m     start_time_s = timeit.default_timer()\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m     result = \u001b[38;5;28;01mawait\u001b[39;00m team.run(task=\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mInvent a short joke about: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtopic\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m      4\u001b[39m     elapsed = timeit.default_timer()-start_time_s\n\u001b[32m      5\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33müîç Generation result (took \u001b[39m\u001b[38;5;132;01m{\u001b[39;00melapsed\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m sec.) resulted in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(result.messages)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m messages.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\dev\\git\\autogen_experiments\\.venv\\Lib\\site-packages\\autogen_agentchat\\teams\\_group_chat\\_base_group_chat.py:340\u001b[39m, in \u001b[36mBaseGroupChat.run\u001b[39m\u001b[34m(self, task, cancellation_token, output_task_messages)\u001b[39m\n\u001b[32m    254\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Run the team and return the result. The base implementation uses\u001b[39;00m\n\u001b[32m    255\u001b[39m \u001b[33;03m:meth:`run_stream` to run the team and then returns the final result.\u001b[39;00m\n\u001b[32m    256\u001b[39m \u001b[33;03mOnce the team is stopped, the termination condition is reset.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    337\u001b[39m \u001b[33;03m    asyncio.run(main())\u001b[39;00m\n\u001b[32m    338\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    339\u001b[39m result: TaskResult | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m340\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m message \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.run_stream(\n\u001b[32m    341\u001b[39m     task=task,\n\u001b[32m    342\u001b[39m     cancellation_token=cancellation_token,\n\u001b[32m    343\u001b[39m     output_task_messages=output_task_messages,\n\u001b[32m    344\u001b[39m ):\n\u001b[32m    345\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(message, TaskResult):\n\u001b[32m    346\u001b[39m         result = message\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\dev\\git\\autogen_experiments\\.venv\\Lib\\site-packages\\autogen_agentchat\\teams\\_group_chat\\_base_group_chat.py:554\u001b[39m, in \u001b[36mBaseGroupChat.run_stream\u001b[39m\u001b[34m(self, task, cancellation_token, output_task_messages)\u001b[39m\n\u001b[32m    550\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(message, GroupChatTermination):\n\u001b[32m    551\u001b[39m     \u001b[38;5;66;03m# If the message contains an error, we need to raise it here.\u001b[39;00m\n\u001b[32m    552\u001b[39m     \u001b[38;5;66;03m# This will stop the team and propagate the error.\u001b[39;00m\n\u001b[32m    553\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m message.error \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m554\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;28mstr\u001b[39m(message.error))\n\u001b[32m    555\u001b[39m     stop_reason = message.message.content\n\u001b[32m    556\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[31mRuntimeError\u001b[39m: ResponseError: model 'qwen3' not found (status code: 404)\nTraceback:\nTraceback (most recent call last):\n\n  File \"c:\\dev\\git\\autogen_experiments\\.venv\\Lib\\site-packages\\autogen_agentchat\\teams\\_group_chat\\_chat_agent_container.py\", line 133, in handle_request\n    async for msg in self._agent.on_messages_stream(self._message_buffer, ctx.cancellation_token):\n    ...<4 lines>...\n            await self._log_message(msg)\n\n  File \"c:\\dev\\git\\autogen_experiments\\.venv\\Lib\\site-packages\\autogen_agentchat\\agents\\_assistant_agent.py\", line 953, in on_messages_stream\n    async for inference_output in self._call_llm(\n    ...<15 lines>...\n            yield inference_output\n\n  File \"c:\\dev\\git\\autogen_experiments\\.venv\\Lib\\site-packages\\autogen_agentchat\\agents\\_assistant_agent.py\", line 1107, in _call_llm\n    model_result = await model_client.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^\n    ...<4 lines>...\n    )\n    ^\n\n  File \"c:\\dev\\git\\autogen_experiments\\.venv\\Lib\\site-packages\\autogen_ext\\models\\ollama\\_ollama_client.py\", line 646, in create\n    result: ChatResponse = await future\n                           ^^^^^^^^^^^^\n\n  File \"C:\\Users\\jmkuhnigk\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\asyncio\\futures.py\", line 286, in __await__\n    yield self  # This tells Task to wait for completion.\n    ^^^^^^^^^^\n\n  File \"C:\\Users\\jmkuhnigk\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\asyncio\\tasks.py\", line 375, in __wakeup\n    future.result()\n    ~~~~~~~~~~~~~^^\n\n  File \"C:\\Users\\jmkuhnigk\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\asyncio\\futures.py\", line 199, in result\n    raise self._exception.with_traceback(self._exception_tb)\n\n  File \"C:\\Users\\jmkuhnigk\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\asyncio\\tasks.py\", line 304, in __step_run_and_handle_result\n    result = coro.send(None)\n\n  File \"c:\\dev\\git\\autogen_experiments\\.venv\\Lib\\site-packages\\ollama\\_client.py\", line 854, in chat\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n    ...<14 lines>...\n    )\n    ^\n\n  File \"c:\\dev\\git\\autogen_experiments\\.venv\\Lib\\site-packages\\ollama\\_client.py\", line 692, in _request\n    return cls(**(await self._request_raw(*args, **kwargs)).json())\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n  File \"c:\\dev\\git\\autogen_experiments\\.venv\\Lib\\site-packages\\ollama\\_client.py\", line 636, in _request_raw\n    raise ResponseError(e.response.text, e.response.status_code) from None\n\nollama._types.ResponseError: model 'qwen3' not found (status code: 404)\n"
     ]
    }
   ],
   "source": [
    "async def invent_joke(topic: str):\n",
    "    start_time_s = timeit.default_timer()\n",
    "    result = await team.run(task=f\"Invent a short joke about: {topic}\")\n",
    "    elapsed = timeit.default_timer()-start_time_s\n",
    "    print(f\"üîç Generation result (took {elapsed:.2f} sec.) resulted in {len(result.messages)} messages.\")\n",
    "    return result  # Return the result for further exploration\n",
    "\n",
    "# Run the deep dive\n",
    "result = await invent_joke(topic)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f53d535",
   "metadata": {},
   "source": [
    "Explore Results\n",
    "==============="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3254dbe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the full result structure\n",
    "print(\"Result type:\", type(result))\n",
    "\n",
    "# print(\"\\nResult attributes:\")\n",
    "# print(get_attributes(result))\n",
    "# print(\"\\nMessage attributes:\")\n",
    "# print(get_attributes(result.messages[-1]))\n",
    "\n",
    "print(\"Message Sequence:\")\n",
    "for i, m in enumerate(result.messages):\n",
    "    print(f\"\\n----------------------------------------------------------------\")\n",
    "    print(f\"MESSAGE {i} [{m.source}]:\\n{m.content}\") # type: ignore\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35292aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up resources\n",
    "await model_client.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14cca613",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from autogen_agentchat.messages import BaseTextChatMessage\n",
    "\n",
    "print(\"Stop reason:\", result.stop_reason)\n",
    "assert isinstance(result.messages[-1], BaseTextChatMessage)\n",
    "\n",
    "print(\"Message Sequence:\")\n",
    "for m in result.messages:\n",
    "    assert isinstance(m, BaseTextChatMessage), f\"Unexpected message type: {type(m)}\"\n",
    "    message = m.content\n",
    "    print(f\" - {m.source}: {message}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "research-assistant",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
